{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+Sxhpf4huNTLhZ8/D3jR/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/docheem/NLP-Portfolio/blob/main/PR_Applying_Transformers_to_Legal_and_Financial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Applying Transformers to Legal and Financial\n",
        "\n",
        "Text-To-Text Transfer Transformer \n",
        "\n",
        "We will go through the concepts and architecture of the T5 transformer model. We will then apply T5 to summarizing documents with Hugging Face models\n",
        "\n"
      ],
      "metadata": {
        "id": "tz5G-EcXl6SW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B89mdSHAlqTD",
        "outputId": "622d85dd-0c42-4c21-f22a-b8f7fc681b65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.26.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece==0.1.94 in /usr/local/lib/python3.9/dist-packages (0.1.94)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece==0.1.94\n",
        "\n",
        "display_architecture = False\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json"
      ],
      "metadata": {
        "id": "aK1alXqqo7G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the tokenizer, generation, and configuration classes\n",
        "\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n"
      ],
      "metadata": {
        "id": "ZTO78Vm-pFtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the T5-large conditional generation model \n",
        "# to generate text and the T5-largetokenizer\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-large')\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqoTYUVUpFx9",
        "outputId": "1dda3840-7bcc-40ee-812a-09b845266aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# device on which torch tensors will be allocated\n",
        "device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "-RGuPlbPpqTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring the architecture of the T5 model"
      ],
      "metadata": {
        "id": "-R26Drfrp4Cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if display_architecture==True:\n",
        "  print(model.config)\n",
        "\n",
        "print(model.config)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuioQpYpp48A",
        "outputId": "d5eb8b26-632b-42bb-f46a-43dbfca8dd1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T5Config {\n",
            "  \"_name_or_path\": \"t5-large\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 4096,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 24,\n",
            "  \"num_heads\": 16,\n",
            "  \"num_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that T5:\n",
        "\n",
        "- Implements the beam search algorithm, which will expand the four most significant text completion predictions\n",
        "- Applies early stopping when num_beam sentences are completed per batch\n",
        "- Makes sure not to repeat ngrams equal to no_repeat_ngram_size\n",
        "- Controls the length of the samples with min_length and max_length\n",
        "- Applies a length penalty\n",
        "- Vocabulary size is a topic in itself. Too much vocabulary will lead to sparse representations. On the other hand, too little vocabulary will distort the NLP tasks."
      ],
      "metadata": {
        "id": "PWpWRiXVqgfA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also see the details of the transformer stacks by simply printing the model"
      ],
      "metadata": {
        "id": "9nhAK7hPrUCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print(model)"
      ],
      "metadata": {
        "id": "oUsy6Hndp5Dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(model.encoder)"
      ],
      "metadata": {
        "id": "d_yqvsAPp5OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(model.decoder)"
      ],
      "metadata": {
        "id": "b50vwgqVp5QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(model.forward)"
      ],
      "metadata": {
        "id": "sxQhOutKp5TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Summarizing documents with T5-large\n",
        "\n",
        "We will create a summarizing function that we can call with any text we wish to summarize. We will summarize legal and financial examples."
      ],
      "metadata": {
        "id": "VeJW1z-ksYA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a summarization function\n",
        "\n",
        "def summarize(text,ml):\n",
        "\n",
        "  # stripped of the \\n character or cleaning up\n",
        "  # removes any extra spaces from the beginning and end of the text. \n",
        "  # Then, it replaces any new line characters (\"\\n\") with nothing.\n",
        "  preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
        "\n",
        "  # apply the innovative T5 task prefix summarize to the input text\n",
        "  # We're going to use a special code to help the robot understand \n",
        "  # that we want it to summarize the story. We'll add the code 'summarize:' \n",
        "  # to the beginning of the story, so the robot knows what we want it to do\n",
        "\n",
        "  t5_prepared_Text = \"summarize: \"+preprocess_text\n",
        "\n",
        "  # Let's check if the story looks right with the special code added.\n",
        "  # We'll print out the story on the computer screen, so we can see \n",
        "  # what the robot is working with\n",
        "\n",
        "  print (\"Preprocessed and prepared text: \\n\", t5_prepared_Text)\n",
        "\n",
        "   # Now that we've cleaned up the story and added the special code, \n",
        "   # we need to turn it into a code that our robot can understand better.\n",
        "   # We'll use a special tool called a 'tokenizer' to do this.\n",
        "\n",
        "  tokenized_text = tokenizer.encode(t5_prepared_Text, \n",
        "                                    \n",
        "                                    return_tensors = \"pt\").to(device)\n",
        "\n",
        "  # Summary\n",
        "  # uses a \"model\" (which is like a really smart robot program) \n",
        "  # to generate a summary of the text\n",
        "  # The settings used (such as \"num_beams\" and \"max_length\") \n",
        "  # help make sure the summary is a good length and makes sense.\n",
        "  summary_ids = model.generate(tokenized_text,\n",
        "                               \n",
        "                                      num_beams = 4,\n",
        "                               \n",
        "                                      no_repeat_ngram_size = 2,\n",
        "                               \n",
        "                                      min_length = 30,\n",
        "                               \n",
        "                                      max_length = ml,\n",
        "                               \n",
        "                                      early_stopping = True)\n",
        "  \n",
        "  # The generated output will now decoded with the tokenizer\n",
        "  # This line takes the summary that the robot generated \n",
        "  # (which is still in token ID form) and turns it back into \n",
        "  # regular words that we can read.\n",
        "\n",
        "  output = tokenizer.decode(summary_ids[0],\n",
        "                            \n",
        "                            skip_special_tokens = True)\n",
        "  \n",
        "  # tells the function to return the summary as the output, \n",
        "  # so we can use it for whatever we need\n",
        "\n",
        "  return output\n",
        "\n"
      ],
      "metadata": {
        "id": "8zpzTEAlsUn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The encoded text is ready to be sent to the model to generate a summary.\n",
        "\n",
        "\n",
        "The input text is the beginning of the Project Gutenberg e-book containing the Declaration of Independence of the United States of America:"
      ],
      "metadata": {
        "id": "et-xu9MbuomB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"\"\"\n",
        "The United States Declaration of Independence was the first Etext\n",
        "released by Project Gutenberg, early in 1971.  The title was stored\n",
        "in an emailed instruction set which required a tape or diskpack be\n",
        "hand mounted for retrieval.  The diskpack was the size of a large\n",
        "cake in a cake carrier, cost $1500, and contained 5 megabytes, of\n",
        "which this file took 1-2%.  Two tape backups were kept plus one on\n",
        "paper tape.  The 10,000 files we hope to have online by the end of\n",
        "2001 should take about 1-2% of a comparably priced drive in 2001.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Q5neFZCWsUqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can call our summarize function and send the text\n",
        "# we want to summarize and the maximum length of the summary\n",
        "\n",
        "print(\"Number of characters:\",len(text))\n",
        "\n",
        "summary = summarize(text,50)\n",
        "\n",
        "\n",
        "print (\"\\n\\nSummarized text: \\n\",summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMEbuYBVsUta",
        "outputId": "0b758c8f-a8e6-4af7-92e1-b1a571a257d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of characters: 534\n",
            "Preprocessed and prepared text: \n",
            " summarize: The United States Declaration of Independence was the first Etextreleased by Project Gutenberg, early in 1971.  The title was storedin an emailed instruction set which required a tape or diskpack behand mounted for retrieval.  The diskpack was the size of a largecake in a cake carrier, cost $1500, and contained 5 megabytes, ofwhich this file took 1-2%.  Two tape backups were kept plus one onpaper tape.  The 10,000 files we hope to have online by the end of2001 should take about 1-2% of a comparably priced drive in 2001.\n",
            "\n",
            "\n",
            "Summarized text: \n",
            " the united states declaration of independence was the first etext published by project gutenberg, early in 1971. the 10,000 files we hope to have online by the end of2001 should take about 1-2% of a comparably priced drive in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Bill of Rights sample"
      ],
      "metadata": {
        "id": "NiOAhzx42fue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bill of Rights sample\n",
        "\n",
        "text =\"\"\"\n",
        "No person shall be held to answer for a capital, or otherwise infamous crime,\n",
        "unless on a presentment or indictment of a Grand Jury, except in cases arising\n",
        "in the land or naval forces, or in the Militia, when in actual service\n",
        "in time of War or public danger; nor shall any person be subject for\n",
        "the same offense to be twice put in jeopardy of life or limb;\n",
        "nor shall be compelled in any criminal case to be a witness against himself,\n",
        "nor be deprived of life, liberty, or property, without due process of law;\n",
        "nor shall private property be taken for public use without just compensation.\n",
        "\n",
        "\"\"\"\n",
        "print(\"Number of characters:\",len(text))\n",
        "\n",
        "summary=summarize(text,50)\n",
        "\n",
        "\n",
        "print (\"\\n\\nSummarized text: \\n\",summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtaf4iDpsUwC",
        "outputId": "77957628-ae05-47c5-ad77-765995214e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of characters: 591\n",
            "Preprocessed and prepared text: \n",
            " summarize: No person shall be held to answer for a capital, or otherwise infamous crime,unless on a presentment or indictment of a Grand Jury, except in cases arisingin the land or naval forces, or in the Militia, when in actual servicein time of War or public danger; nor shall any person be subject forthe same offense to be twice put in jeopardy of life or limb;nor shall be compelled in any criminal case to be a witness against himself,nor be deprived of life, liberty, or property, without due process of law;nor shall private property be taken for public use without just compensation.\n",
            "\n",
            "\n",
            "Summarized text: \n",
            " no person shall be held to answer for a capital, or otherwise infamous crime, unless ona presentment or indictment ofa Grand Jury. nor shall any person be subject for the same offense to be twice put\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A corporate law sample"
      ],
      "metadata": {
        "id": "VqLTBYde24_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Montana Corporate Law\n",
        "# https://corporations.uslegal.com/state-corporation-law/montana-corporation-law/#:~:text=Montana%20Corporation%20Law,carrying%20out%20its%20business%20activities.\n",
        "\n",
        "text =\"\"\"The law regarding corporations prescribes that a corporation can be incorporated in the state of Montana to serve any lawful purpose.  In the state of Montana, a corporation has all the powers of a natural person for carrying out its business activities.  The corporation can sue and be sued in its corporate name.  It has perpetual succession.  The corporation can buy, sell or otherwise acquire an interest in a real or personal property.  It can conduct business, carry on operations, and have offices and exercise the powers in a state, territory or district in possession of the U.S., or in a foreign country.  It can appoint officers and agents of the corporation for various duties and fix their compensation.\n",
        "The name of a corporation must contain the word “corporation” or its abbreviation “corp.”  The name of a corporation should not be deceptively similar to the name of another corporation incorporated in the same state.  It should not be deceptively identical to the fictitious name adopted by a foreign corporation having business transactions in the state.\n",
        "The corporation is formed by one or more natural persons by executing and filing articles of incorporation to the secretary of state of filing.  The qualifications for directors are fixed either by articles of incorporation or bylaws.  The names and addresses of the initial directors and purpose of incorporation should be set forth in the articles of incorporation.  The articles of incorporation should contain the corporate name, the number of shares authorized to issue, a brief statement of the character of business carried out by the corporation, the names and addresses of the directors until successors are elected, and name and addresses of incorporators.  The shareholders have the power to change the size of board of directors.\n",
        "\"\"\"\n",
        "print(\"Number of characters:\",len(text))\n",
        "\n",
        "summary = summarize(text,50)\n",
        "\n",
        "\n",
        "print (\"\\n\\nSummarized text: \\n\",summary)\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z95j1At_sUyp",
        "outputId": "7091701b-b1a5-4886-ffbe-1d789969ceb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of characters: 1816\n",
            "Preprocessed and prepared text: \n",
            " summarize: The law regarding corporations prescribes that a corporation can be incorporated in the state of Montana to serve any lawful purpose.  In the state of Montana, a corporation has all the powers of a natural person for carrying out its business activities.  The corporation can sue and be sued in its corporate name.  It has perpetual succession.  The corporation can buy, sell or otherwise acquire an interest in a real or personal property.  It can conduct business, carry on operations, and have offices and exercise the powers in a state, territory or district in possession of the U.S., or in a foreign country.  It can appoint officers and agents of the corporation for various duties and fix their compensation.The name of a corporation must contain the word “corporation” or its abbreviation “corp.”  The name of a corporation should not be deceptively similar to the name of another corporation incorporated in the same state.  It should not be deceptively identical to the fictitious name adopted by a foreign corporation having business transactions in the state.The corporation is formed by one or more natural persons by executing and filing articles of incorporation to the secretary of state of filing.  The qualifications for directors are fixed either by articles of incorporation or bylaws.  The names and addresses of the initial directors and purpose of incorporation should be set forth in the articles of incorporation.  The articles of incorporation should contain the corporate name, the number of shares authorized to issue, a brief statement of the character of business carried out by the corporation, the names and addresses of the directors until successors are elected, and name and addresses of incorporators.  The shareholders have the power to change the size of board of directors.\n",
            "\n",
            "\n",
            "Summarized text: \n",
            " a corporation can be incorporated in the state of Montana to serve any lawful purpose. the corporation has perpetual succession and can sue and be sued in its corporate name. it can conduct business, carry on operations, and have offices\n"
          ]
        }
      ]
    }
  ]
}